#!/usr/bin/env python3
# -----------------------------------------------------------------------------
# Copyright (c) 2025
# Licensed under the GNU General Public License, version 2
# -----------------------------------------------------------------------------

import csv
import json
import os
import sys
import argparse
import time
import requests
import re

from tqdm import tqdm
from jinja2 import Environment, FileSystemLoader, select_autoescape
from lxml import etree

# Local settings
from companies_house_settings import (
    scp_destinations,
    fca_user,
    fca_api_key,
    companies_house_snapshot_file,
    CASH_ALERT_VALUE,
    OTHER_ALERT_VALUE
)

DATA_DIRECTORY = "companies_house_data"
COMPANIES_HOUSE_SNAPSHOT = os.path.join(DATA_DIRECTORY, companies_house_snapshot_file)
SIC_CODES_FILE = "sic_codes.json"
ACCOUNTS_LOOKUP_TABLE_FILE = os.path.join(DATA_DIRECTORY, "company_accounts_lookup.json") 
INDEX_DB_FILE = os.path.join(DATA_DIRECTORY, "index_of_company_SICs.db")

COMPANIES_HOUSE_URL = "https://find-and-update.company-information.service.gov.uk/company/{}"
LOGO_URL = "https://taxpolicy.org.uk/wp-content/uploads/elementor/thumbs/logo-in-banner-tight-1-quqdc4qw34zmeg68ggs0psp29qcsftxn2pptrt1h6m.png"
LOGO_LINK = "https://taxpolicy.org.uk"


NUM_WORKER_THREADS = 1000
DEBUG_LIMIT = None  # e.g. set to 1000 for debugging. None for no limit

def load_json_file(filepath, description):
    """Load JSON from a file and exit if not found/invalid."""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        sys.exit(f"Error: Failed to load '{filepath}'. Ensure it exists and is valid JSON.")

def read_with_progress(file, pbar):
    """Generator that reads lines from a file and updates the progress bar."""
    for line in file:
        pbar.update(len(line.encode("utf-8")))
        yield line

def read_csv_with_progress(filepath, desc):
    """Generator to read CSV rows with a progress bar."""
    file_size = os.path.getsize(filepath)
    with open(filepath, "r", encoding="utf-8") as f:
        first_line = f.readline()
        headers = [h.strip() for h in first_line.strip().split(",")]
        reader = csv.DictReader(
            read_with_progress(f, tqdm(total=file_size - len(first_line.encode("utf-8")),
                                       unit="B", unit_scale=True, desc=desc)),
            fieldnames=headers
        )
        yield from reader



def extract_ixbrl_data(ixbrl_path):
    """
    Extract 'financials', 'dormant' and 'name' from an iXBRL file using lxml.
    """
    data = {"financials": {}, "dormant": None, "name": None}
    try:
        with open(ixbrl_path, "rb") as f:
            tree = etree.parse(f)
    except Exception as e:
        print(f"Error parsing {ixbrl_path}: {e}")
        return data

    try:
        # Grab all ix:nonFraction elements
        non_fracs = tree.xpath('//*[local-name()="nonFraction"]')
    
        for elem in non_fracs:
            tag = elem.get("name", "")
            local_tag = tag.split(":", 1)[-1] if ":" in tag else tag
            val = "".join(elem.itertext()).strip()
            data["financials"][local_tag] = val
    except Exception as e:
        print(f"Error extracting ix:nonFraction from {ixbrl_path}: {e}")

    try:
        non_num = tree.xpath('//*[local-name()="nonNumeric" and '
                             '(substring-after(@name, ":")="EntityDormantTruefalse" or '
                             ' substring-after(@name, ":")="EntityCurrentLegalOrRegisteredName")]')
        for elem in non_num:
            tag = elem.get("name", "")
            local_tag = tag.split(":", 1)[-1] if ":" in tag else tag
            val = "".join(elem.itertext()).strip()
            if local_tag == "EntityDormantTruefalse":
                data["dormant"] = "Yes" if (val.lower() == "true") else ""
            elif local_tag == "EntityCurrentLegalOrRegisteredName":
                data["name"] = val
                
    except Exception as e:
        print(f"Error extracting ix:nonNumeric from {ixbrl_path}: {e}")

    return data


def filter_for_large_companies(companies, lookup_table, cash_only=False, nofilter=False):
    
    if nofilter:
        print("-nofilter - so returning all matching companies, not filtering for large balance sheets")
        all_results = []
        for co in companies:
            all_results.append({
                "CompanyName": co.get("CompanyName", "").strip(),
                "CompanyNumber": co.get("CompanyNumber", "").strip(),
                "SIC1": co["Data"].get("SICCode.SicText_1", ""),
                "SIC2": co["Data"].get("SICCode.SicText_2", ""),
                "SIC3": co["Data"].get("SICCode.SicText_3", ""),
                "SIC4": co["Data"].get("SICCode.SicText_4", ""),
                "Cash": "",
                "SuspectData": [],
                "Dormant": ""
            })
        return all_results
    
    """Filter companies with large balance sheets."""
    suspect_companies = []

    for company in tqdm(companies, desc="Filtering for large balance sheets"):
        company_name = company.get("CompanyName", "").strip()
        company_number = company.get("CompanyNumber", "").strip()
        regulatory_status = company.get("regulatory_status", [])

        if regulatory_status:
            continue

        ixbrl_file_path = lookup_table.get(company_number)
        if not ixbrl_file_path:
            continue

        data = extract_ixbrl_data(ixbrl_file_path)
        
        fin_data = data["financials"]
        cash_value = 0
        suspect_data = []

        # search for suspect cash balances over CASH_ALERT_VALUE
        # and if not run with the -cashonly option, search for any balance over OTHER_ALERT_VALUE
        for k, v in fin_data.items():
            try:
                val_num = float(v.replace(",", ""))
            except ValueError:
                # print(f"DEBUG: {co['CompanyNumber']} - Invalid value for {k}: '{v}'")
                continue

            if "cash" in k.lower():
                cash_value = max(int(val_num), cash_value)
                # print(f"DEBUG: {co['CompanyNumber']} - CashBankOnHand updated from {previous_cash} to {cash_value}")
            elif val_num > OTHER_ALERT_VALUE and not cash_only:
                suspect_data.append((k, val_num))

        if (cash_value > CASH_ALERT_VALUE) or suspect_data:
            suspect_companies.append({
                "CompanyName": company_name,
                "CompanyNumber": company_number,
                "SIC1": company["Data"].get("SICCode.SicText_1", ""),
                "SIC2": company["Data"].get("SICCode.SicText_2", ""),
                "SIC3": company["Data"].get("SICCode.SicText_3", ""),
                "SIC4": company["Data"].get("SICCode.SicText_4", ""),
                "Cash": cash_value,
                "SuspectData": suspect_data,
                "Dormant": data.get("dormant", "")
            })

    suspect_companies.sort(key=lambda x: x["Cash"], reverse=True)
    if cash_only:
        print(f"Found {len(suspect_companies)} with large cash balances (I was run with the -cashonly option)")
    else:
        print(f"Found {len(suspect_companies)} with large balance sheets")
        
    return suspect_companies

def filter_for_reg_status(companies, reg):
    if reg:
        companies = add_reg_status(companies)
        companies = remove_regulated_companies(companies)
        
    return companies

def search_fca_register(firm_name, progress_info, max_retries=3, backoff_factor=2):
    """
    Searches the FCA register for `firm_name`, returns a list of matching data dicts.
    """
    url = "https://register.fca.org.uk/services/V0.1/Search"
    headers = {
        "x-auth-email": fca_user,
        "x-auth-key": fca_api_key,
        "Content-Type": "application/json"
    }
    params = {"q": firm_name, "type": "firm"}

    wait_time = 1
    for attempt in range(1, max_retries + 2):
        try:
            print(f"{progress_info} '{firm_name}' (attempt {attempt}/{max_retries+1})")
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json().get("Data")
                if isinstance(data, list):
                    print(f"Found {len(data)} record(s).")
                    return data
                else:
                    # print(f"No data found.")
                    return []
            elif r.status_code == 429:
                print("Warning: Rate limit. Sleeping 65s...")
                time.sleep(65)
            elif 500 <= r.status_code < 600: 
                print(f"Server error {r.status_code} for '{firm_name}'. Wait {wait_time}s...")
                time.sleep(wait_time)
                wait_time *= backoff_factor
            else:
                print(f"Error: HTTP {r.status_code} for '{firm_name}'. Skipping.")
                return []
        except requests.exceptions.Timeout:
            print(f"Timeout for '{firm_name}'. Wait {wait_time}s...")
            time.sleep(wait_time)
            wait_time *= backoff_factor
        except requests.exceptions.RequestException as e:
            print(f"Request error for '{firm_name}': {e}")
            time.sleep(wait_time)
            wait_time *= backoff_factor
    print(f"Failed to retrieve data for '{firm_name}' after {max_retries} attempts.")
    return []

def add_reg_status(companies):
    """
    Adds regulatory status to each company and prunes the regulatory information
    to include only entries that match the company's normalized name.
    """
    for i, co in enumerate(companies, 1):
        cname = co.get("CompanyName", "").strip()
        cnum = co.get("CompanyNumber", "").strip()
        if not cname or not cnum:
            print(f"[{i}/{len(companies)}] Missing name/number - skipping.")
            co["regulatory_status"] = []
            continue

        # Fetch regulatory status
        reg_status = search_fca_register(cname, f"[{i}/{len(companies)}]")
        
        # Prune regulatory status
        norm_cname = normalize_name(cname)
        pruned_status = []
        for entry in reg_status:
            entry_name = entry.get("Name", "")
            norm_entry_name = normalize_name(entry_name)
            if norm_entry_name == norm_cname:
                pruned_status.append(entry)
            else:
                pass
                # print(f"{cname}: Deleting regulatory entry '{entry_name}' as it does not match normalized name.")
        
        co["regulatory_status"] = pruned_status
    return companies


def normalize_name(name):
    name = name.lower().strip()
    name = re.sub(r"\s*\([^)]*\)\s*", " ", name).strip()  # Removes parentheses and contents
    name = name.replace("limited", "ltd").replace(".", "").replace(",", "")
    return name

def remove_regulated_companies(companies):
    init_count = len(companies)
    unreg = [c for c in companies if not c.get("regulatory_status")]
    print(f"Filtered out {init_count - len(unreg)} regulated companies. {len(unreg)} remain.")
    return unreg

def create_html(suspect_companies, table_title, descriptor):
    filename = f"output/results-{descriptor}.html"
    os.makedirs("output", exist_ok=True)

    env = Environment(
        loader=FileSystemLoader("."),
        autoescape=select_autoescape(["html", "xml"])
    )

    def format_number(val):
        try:
            return "{:,}".format(float(val))
        except:
            return val
    env.filters["format_number"] = format_number

    template_str = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>TABLE_TITLE</title>
    <!-- Google Fonts: Poppins -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet">
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- DataTables CSS -->
    <link rel="stylesheet" href="https://cdn.datatables.net/1.13.4/css/dataTables.bootstrap5.min.css">
    <!-- Favicon -->
    <link rel="icon" href="{{ logo_url }}" sizes="32x32" type="image/jpeg">

    <style>
        body {
            font-family: 'Poppins', sans-serif;
            color: #000000;
        }
        .container {
            margin-top: 50px;
        }
        /* Header Styling */
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            background-color: #1133AF; 
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #FFFFFF;
            margin: 0;
            font-weight: 600;
        }
        /* Logo Styling */
        .logo {
            height: 60px;
        }
        /* Table Header Styling */
        table.dataTable thead th {
            background-color: #1133AF;
            color: white;
        }
        /* Table Row Hover and Zebra Striping */
        table.dataTable tbody tr:nth-child(odd) {
            background-color: #f9f9f9;
        }
        table.dataTable tbody tr:hover {
            background-color: #D3D3D3 !important;
        }
        /* Links Styling */
        a {
            color: #1133AF;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        /* DataTables Length and Filter Styling */
        .dataTables_wrapper .dataTables_length,
        .dataTables_wrapper .dataTables_filter {
            color: #000000;
        }
        /* Ensure the Cash column is right-aligned for better readability */
        td:nth-child(4), th:nth-child(4) {
            text-align: left;
        }
        /* SIC Cell Styling */
        .sic-cell span {
            display: block;
            max-width: 300px; /* Adjust as needed */
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            cursor: pointer;
        }
        /* Added CSS for wrapping long suspect items */
        .item-cell {
            max-width: 200px; /* Adjust the width as needed */
            word-wrap: break-word;
            white-space: normal;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="header" id="main-header">
            <h1>TABLE_TITLE</h1>
            <a href="{{ logo_link }}" target="_blank">
                <img src="{{ logo_url }}" alt="Company Logo" class="logo">
            </a>
        </div>

        <table id="suspectCompaniesTable" class="table table-striped table-bordered" style="width:100%">
            <thead>
                <tr>
                    <th>Company Name</th>
                    <th>Company Number</th>
                    <th>Dormant</th>
                    {% if descriptor != "all" %}
                        <th>SICs</th>
                    {% endif %}
                    <th>Cash</th>
                    {% for i in range(1, max_suspects + 1) %}
                        <th>Item {{ i }}</th>
                        <th>Value {{ i }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody>
                {% for co in companies %}
                <tr>
                    <td>{{ co.CompanyName }}</td>
                    <td>
                        <a href="{{ ch_url.format(co.CompanyNumber) }}" target="_blank">
                            {{ co.CompanyNumber }}
                        </a>
                    </td>
                    <td>{{ co.Dormant }}</td>
                    
                    {% if descriptor != "all" %}
                        <td class="sic-cell">
                            {% for sic in [co.SIC1, co.SIC2, co.SIC3, co.SIC4] %}
                                {% if sic %}
                                    <span data-bs-toggle="tooltip" data-bs-placement="top" title="{{ sic }}">
                                        {{ sic[:25] }}{% if sic|length > 25 %}...{% endif %}
                                    </span>
                                {% endif %}
                            {% endfor %}
                        </td>
                    {% endif %}

                    <td data-sort="{{ co.Cash }}">{{  "{:,.0f}".format(co.Cash|int)  }}</td>

                    {% for item, val in co.SuspectData %}
                        <td class="item-cell">{{ item }}</td>
                        <td>{{ "{:,.0f}".format(val|int) }}</td>
                    {% endfor %}

                    {% for _ in range(max_suspects - co.SuspectData|length) %}
                        <td></td>
                        <td></td>
                    {% endfor %}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <!-- Bootstrap JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- DataTables JS -->
    <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
    <script src="https://cdn.datatables.net/1.13.4/js/dataTables.bootstrap5.min.js"></script>
    <!-- Scroller Extension JS -->
    <script src="https://cdn.datatables.net/scroller/2.1.2/js/dataTables.scroller.min.js"></script>

    <script>
        $(document).ready(function() {
            $('#suspectCompaniesTable').DataTable({
                scrollY: "800px",
                scrollX: true,
                scroller: true,
                paging: false,
                searching: false,
                order: [[ 3, "desc" ]],
                lengthMenu: [10, 25, 50, 100, 4021],
                columnDefs: [
                    {
                        targets: 3,
                        type: "num",
                        render: function (data, type, row) {
                            // So the table sorts numerically by the raw value in data-sort
                            if (type === 'sort') {
                                return data;
                            }
                            return data;
                        }
                    }
                ],
                language: {
                    search: "Filter records:",
                    lengthMenu: "Show _MENU_ entries",
                    info: "Showing _START_ to _END_ of _TOTAL_ companies",
                    infoEmpty: "No companies available",
                    infoFiltered: "(filtered from _MAX_ total companies)"
                }
            });

            // Initialize Bootstrap tooltips
            var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'))
            tooltipTriggerList.map(function (tooltipTriggerEl) {
                return new bootstrap.Tooltip(tooltipTriggerEl)
            });
        });

        // Hide the header if embedded in an iframe
        function checkIfEmbedded() {
            if (window.self !== window.top) {
                var header = document.getElementById('main-header');
                if (header) {
                    header.style.display = 'none';
                }
            }
        }
        document.addEventListener("DOMContentLoaded", checkIfEmbedded);
    </script>
</body>
</html>
"""
    template_str = template_str.replace("TABLE_TITLE", table_title)
    template = env.from_string(template_str)

    if "SuspectData" in suspect_companies[0]:
        max_suspects = max((len(c["SuspectData"]) for c in suspect_companies), default=0)
    else:
        max_suspects = 0
        
    html_out = template.render(
        companies=suspect_companies,
        max_suspects=max_suspects,
        ch_url=COMPANIES_HOUSE_URL,
        logo_url=LOGO_URL,
        logo_link=LOGO_LINK,
        descriptor=descriptor
    )
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_out)
    except IOError as e:
        print(f"Error writing {filename}: {e}")
    return filename

def create_csv(suspect_companies, descriptor):
    filename = f"output/results-{descriptor}.csv"
    os.makedirs("output", exist_ok=True)
    with open(filename, "w", encoding="utf-8", newline="") as out:
        w = csv.writer(out)
        # Basic header
        hdr = ["Company Name", "Company Number", "Cash"]
        if descriptor == "all":
            hdr += ["Dormant"]
            max_items = 0
        else:
            hdr += ["SIC1", "SIC2", "SIC3", "SIC4"]
            max_items = max(len(c["SuspectData"]) for c in suspect_companies) if suspect_companies else 0
            for i in range(max_items):
                hdr += [f"Item {i+1}", f"Value {i+1}"]
        w.writerow(hdr)

        for co in suspect_companies:
            row = [co["CompanyName"], co["CompanyNumber"], co["Cash"]]
            if descriptor == "all":
                row.append(co.get("Dormant", ""))
            else:
                row.extend([
                    co.get("SIC1",""), co.get("SIC2",""),
                    co.get("SIC3",""), co.get("SIC4","")
                ])
                for (item, val) in co["SuspectData"]:
                    row += [item, val]
                # pad out empty columns
                needed = max_items - len(co["SuspectData"])
                row += ["",""] * needed
            w.writerow(row)
    return filename

def export_results(companies, html_title, descriptor):
    if not companies:
        print("\nNo results - nothing to export.")
        return
    html_file = create_html(companies, html_title, descriptor)
    csv_file = create_csv(companies, descriptor)
    
    if scp_destinations:
        for dest in scp_destinations:
            os.system(f"scp {html_file} {dest}")
            os.system(f"scp {csv_file} {dest}")
    else:
        print(f"Exported to:\n  {html_file}\n  {csv_file}\n")


def indexed_search(search_type, search_terms):
    import sqlite3

    print(f"Indexed {search_type} search for '{search_terms}'...")

    conn = sqlite3.connect(INDEX_DB_FILE)
    cursor = conn.cursor()

    
    if search_type == "address":
        # Use LIKE operator with wildcards for partial matching
        query = '''
            SELECT CompanyNumber, CompanyName, Data
            FROM Companies
            WHERE FullAddress LIKE ?
        '''
        search_pattern = f"%{search_terms.strip().lower()}%"
        cursor.execute(query, (search_pattern,))
        
    elif search_type == "SIC":
        placeholders = ','.join(['?'] * len(search_terms))
        query = f'''
            SELECT DISTINCT c.CompanyNumber, c.CompanyName, c.Data
            FROM SICCodes s
            JOIN Companies c ON s.CompanyNumber = c.CompanyNumber
            WHERE s.SICCode IN ({placeholders})
        '''
        cursor.execute(query, tuple(search_terms))
    else:
        print("Error - unknown indexed search type")
        sys.exit(1)
        
    rows = cursor.fetchall()

    matched_companies = []
    for row in rows:
        cnum, cname, data_json = row
        data = json.loads(data_json)
                    
        matched_companies.append({
            "CompanyName": cname,
            "CompanyNumber": cnum,
            "Data": data,
        })

    conn.close()

    print(f"Found {len(matched_companies)} matching companies for {search_type} search.")
    return matched_companies



def search_for_sic_code_matches(sic_list, all_sic_codes):
    
    print("Searching for the following SIC codes:")
    for code in sic_list:
        print(f"{code}: {all_sic_codes[code]}")
    
    if os.path.exists(INDEX_DB_FILE):
        companies = indexed_search("SIC", sic_list)
        return companies
    
    print("You can speed this up by creating an index and running with -sicindex")

    companies = []
    total_searched = 0

    sic_fields = ["SICCode.SicText_1", "SICCode.SicText_2", "SICCode.SicText_3", "SICCode.SicText_4"]
    desc = "Finding matching SIC companies"
    if not os.path.exists(COMPANIES_HOUSE_SNAPSHOT):
        sys.exit(f"Error: Snapshot '{COMPANIES_HOUSE_SNAPSHOT}' not found.")

    for row in read_csv_with_progress(COMPANIES_HOUSE_SNAPSHOT, desc):
        total_searched += 1
        if DEBUG_LIMIT and total_searched > DEBUG_LIMIT:
            break
        matched_codes = []
        for field in sic_fields:
            sic_val = row.get(field, "").strip()
            if sic_val:
                code = sic_val[:5]
                if code.isdigit() and code in sic_list:
                    matched_codes.append(code)
        if matched_codes:
            cnum = row.get("CompanyNumber", "").strip()
            cname = row.get("CompanyName", "").strip()
            if cnum:
                companies.append({
                    "CompanyName": cname,
                    "CompanyNumber": cnum,
                    "RelevantSicCodes": matched_codes,
                    "Data": row
                })

    print(f"\nFound {len(companies)} matching SICs from {total_searched} companies")
    return companies



######################################
# multithreaded function for finding ALL companies
# different approach from standard search because it simply goes through all accounts in lookup table

def find_all_cash_rich_companies_fast(lookup_table):
    import threading
    import queue
    import concurrent.futures
    
    
    def worker(company_number, ixbrl_file_path, result_queue, total_companies):
        if not ixbrl_file_path:
            return

        all_accounts_data = extract_ixbrl_data(ixbrl_file_path)
        
        cash = 0
        for key, value in all_accounts_data["financials"].items():
            if "cash" in key.lower():
                clean_str_value = value.replace(",", "")
                try:
                    value_int = int(clean_str_value)
                except ValueError:
                    return
                cash = max(cash, value_int)
        
        if cash == 0:
            return
        
        company_name = all_accounts_data["name"]
        if cash > CASH_ALERT_VALUE:
            result_queue.put({
                "CompanyNumber": company_number,
                "CompanyName": company_name,
                "Dormant": all_accounts_data["dormant"],
                "Cash": value_int
            })


    
    def collector(result_queue, cash_rich_companies, done_event):
        while not done_event.is_set() or not result_queue.empty():
            try:
                item = result_queue.get(timeout=0.1)
                if item is None:
                    # Sentinel received
                    break
                cash_rich_companies.append(item)
            except queue.Empty:
                continue
    
    total_companies = len(lookup_table)
    print(f"Searching all {total_companies} company accounts for cash-rich companies ...")
    
    cash_rich_companies = []
    result_queue = queue.Queue()
    done_event = threading.Event()

    # Start the collector thread
    collector_thread = threading.Thread(target=collector, args=(result_queue, cash_rich_companies, done_event))
    collector_thread.start()

    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKER_THREADS) as executor:
        futures = []
        for company_number, ixbrl_file_path in tqdm(lookup_table.items(), total=total_companies, desc="Searching all companies for high cash balances"):
            futures.append(executor.submit(worker, company_number, ixbrl_file_path, result_queue, total_companies))
        
        # Wait for all futures to complete
        concurrent.futures.wait(futures)

    # All workers are done, signal the collector to finish
    done_event.set()
    collector_thread.join()

    print(f"Found {len(cash_rich_companies)} with large balance sheets")
    return cash_rich_companies

######################


def search_for_address_matches(address_args):
    
    if os.path.exists(INDEX_DB_FILE):
        companies = indexed_search("address", address_args)
        return companies
        
    address_fields = [
        "RegAddress.CareOf",
        "RegAddress.POBox",
        "RegAddress.AddressLine1",
        "RegAddress.AddressLine2",
        "RegAddress.PostTown",
        "RegAddress.County",
        "RegAddress.Country",
        "RegAddress.PostCode"
    ]
    
    address = address_args.strip()
    print(f"Searching for address '{address}'...")
    if not os.path.exists(COMPANIES_HOUSE_SNAPSHOT):
        sys.exit(f"Error: Snapshot '{COMPANIES_HOUSE_SNAPSHOT}' not found.")

    companies = []
    total_searched = 0
    desc = "Finding companies by address"
    for row in read_csv_with_progress(COMPANIES_HOUSE_SNAPSHOT, desc):
        total_searched += 1
        if DEBUG_LIMIT and total_searched > DEBUG_LIMIT:
            break
        
        # Aggregate all address fields into a single string
        full_address = ' '.join([row.get(field, "").lower() for field in address_fields])
        
        if address.lower() in full_address:
            cnum = row.get("CompanyNumber", "").strip()
            cname = row.get("CompanyName", "").strip()
            if cnum:
                companies.append({
                    "CompanyName": cname,
                    "CompanyNumber": cnum,
                    "Data": row
                })

    print(f"Found {len(companies)} from {total_searched} total lines (address search).")

    return companies



def create_index(csv_path=COMPANIES_HOUSE_SNAPSHOT, db_path=INDEX_DB_FILE):
    import sqlite3
    if not os.path.exists(csv_path):
        sys.exit(f"Error: CSV file '{csv_path}' not found.")

    # Remove existing DB if exists
    if os.path.exists(db_path):
        print(f"Creating index and replacing existing database.")
        os.remove(db_path)
    else:
        print(f"Creating index. Database Path: {db_path}")
        
    # Define address fields
    address_fields = [
        "RegAddress.CareOf",
        "RegAddress.POBox",
        "RegAddress.AddressLine1",
        "RegAddress.AddressLine2",
        "RegAddress.PostTown",
        "RegAddress.County",
        "RegAddress.Country",
        "RegAddress.PostCode"
    ]

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Optimize SQLite for faster insertion
    cursor.execute('PRAGMA synchronous = OFF')
    cursor.execute('PRAGMA journal_mode = MEMORY')
    cursor.execute('PRAGMA temp_store = MEMORY')
    cursor.execute('PRAGMA cache_size = 100000')
    cursor.execute('PRAGMA foreign_keys = ON')

    # Create tables
    cursor.execute('''
        CREATE TABLE Companies (
            CompanyNumber TEXT PRIMARY KEY,
            CompanyName TEXT,
            FullAddress TEXT,
            Data TEXT
        )
    ''')

    cursor.execute('''
        CREATE TABLE SICCodes (
            SICCode TEXT,
            CompanyNumber TEXT,
            FOREIGN KEY (CompanyNumber) REFERENCES Companies (CompanyNumber)
        )
    ''')

    # Create index for fast lookup
    cursor.execute('CREATE INDEX idx_sic_code ON SICCodes (SICCode)')
    cursor.execute('CREATE INDEX idx_company_number ON SICCodes (CompanyNumber)')
    cursor.execute('CREATE INDEX idx_full_address ON Companies (FullAddress)')

    conn.commit()

    # Read CSV and populate the database
    with open(csv_path, 'r', newline='', encoding='utf-8') as csvfile:
        # Read the first line to get headers
        first_line = csvfile.readline()
        headers = first_line.strip().split(',')
        # Strip whitespace from each header
        stripped_headers = [header.strip() for header in headers]

        # Initialize DictReader with stripped headers
        reader = csv.DictReader(csvfile, fieldnames=stripped_headers)
        
        # Skip the first line since we've already read the headers
        next(reader, None)

        # Verify required fields
        required_fields = ["CompanyNumber", "CompanyName", "SICCode.SicText_1", 
                           "SICCode.SicText_2", "SICCode.SicText_3", "SICCode.SicText_4"]
        missing_fields = [field for field in required_fields if field not in stripped_headers]
        if missing_fields:
            print(f"Error: Missing required fields in CSV: {missing_fields}")
            sys.exit(1)

        # Count total rows for progress bar
        
        with open(csv_path, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1  # Exclude header
        
        # Reset file pointer to after the header
        csvfile.seek(0)
        next(csvfile)  # Skip the header line
        reader = csv.DictReader(csvfile, fieldnames=stripped_headers)

        batch_size = 10000
        companies_batch = []
        sics_batch = []
        inserted_companies = 0
        inserted_sics = 0

        with tqdm(total=total_rows, desc="Indexing CSV") as pbar:
            for idx, row in enumerate(reader, start=1):
                company_number = row.get("CompanyNumber", "").strip()
                company_name = row.get("CompanyName", "").strip()                
                full_address = ' '.join([row.get(field, "").strip().lower() for field in address_fields])

                if not company_number:
                    pbar.update(1)
                    continue  # Skip entries without CompanyNumber

                # Serialize row to JSON, handle serialization errors
                try:
                    data_json = json.dumps(row)
                except (TypeError, OverflowError) as e:
                    print(f"JSON Serialization Error for CompanyNumber {company_number}: {e}")
                    data_json = "{}"  # Fallback to empty JSON

                # Store company details
                companies_batch.append((
                    company_number,
                    company_name,
                    full_address,
                    data_json
                ))

                # Extract and store SIC codes
                sic_fields = ["SICCode.SicText_1", "SICCode.SicText_2", 
                              "SICCode.SicText_3", "SICCode.SicText_4"]
                for field in sic_fields:
                    sic_val = row.get(field, "").strip()
                    if sic_val:
                        code = sic_val[:5]
                        if code.isdigit():
                            sics_batch.append((code, company_number))

                # Insert in batches
                if idx % batch_size == 0:
                    try:
                        cursor.executemany('INSERT OR IGNORE INTO Companies VALUES (?, ?, ?, ?)', companies_batch)
                        cursor.executemany('INSERT INTO SICCodes VALUES (?, ?)', sics_batch)
                        conn.commit()
                        inserted_companies += len(companies_batch)
                        inserted_sics += len(sics_batch)
                    except sqlite3.Error as e:
                        print(f"SQLite Error at batch {idx // batch_size}: {e}")
                        conn.rollback()
                    finally:
                        companies_batch.clear()
                        sics_batch.clear()

                pbar.update(1)

        # Insert any remaining records
        if companies_batch or sics_batch:
            try:
                cursor.executemany('INSERT OR IGNORE INTO Companies VALUES (?, ?, ?, ?)', companies_batch)
                cursor.executemany('INSERT INTO SICCodes VALUES (?, ?)', sics_batch)
                conn.commit()
                inserted_companies += len(companies_batch)
                inserted_sics += len(sics_batch)
                
            except sqlite3.Error as e:
                print(f"SQLite Error during final batch insertion: {e}")
                conn.rollback()

    conn.close()
    print(f"Indexing complete. Database saved to '{db_path}'.")

def filter_for_dormant(companies, dormant_arg):
    if dormant_arg:
        companies = [company for company in companies if company.get("Dormant") == "Yes"]
        print(f"Of which {len(companies)} are dormant")
        return companies
    else:
        return companies
    

def show_interesting_sics():
    interesting_sics = """
Some interesting SIC codes:

    64191: Banks
    64110: Central banking
    64192: Building societies
    64301: Activities of investment trusts
    64302: Activities of unit trusts
    64303: Activities of venture and development capital companies
    64304: Activities of open-ended investment companies
    64305: Activities of property unit trusts
    64306: Activities of real estate investment trusts
    64910: Financial leasing
    64921: Credit granting by non-deposit taking finance houses and other specialist consumer credit grantors
    64922: Activities of mortgage finance companies
    64929: Other credit granting n.e.c.
    64991: Security dealing on own account
    64992: Factoring
    64999: Financial intermediation not elsewhere classified
    65110: Life insurance
    65120: Non-life insurance
    65201: Life reinsurance
    65202: Non-life reinsurance
    65300: Pension funding
    66120: Security and commodity contracts dealing activities
    66220: Activities of insurance agents and brokers
    66300: Fund management activities

    Run with -sichelp TEXT to see all the SIC codes matching TEXT
    or run with -sichelp ALL to see every SIC code.
"""
    print(interesting_sics)

def main():
    
    class CustomFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
        pass
    
    parser = argparse.ArgumentParser(
        description=(
            "taxpolicy.org.uk\n"
            f"Find companies matching certain criteria with either "
            f"cash > £{CASH_ALERT_VALUE/1e6:.0f}m, or "
            f"other balances > £{OTHER_ALERT_VALUE/1e6:.0f}m.\n"
            f"(c) Dan Neidle, Tax Policy Associates 2025\n"
            f"Licensed under the GNU General Public License, version 2\n"
            
            
        ), formatter_class=CustomFormatter
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-sichelp", nargs="?", const="help", help="Show available SIC codes (optionally filter).")
    group.add_argument("-sic", help="Search by SIC code(s).")
    group.add_argument("-address", help="Search by partial address.")
    group.add_argument("-all", action="store_true", help="Process all companies (will take 40 mins+).")
    group.add_argument("-build", action="store_true", help="Build the accounts lookup table.")
    group.add_argument("-index", action="store_true", help="Build an index for instant SIC/address searches. Will take a few minutes.")

    parser.add_argument("-reg", action="store_true", help="Filter out regulated companies.")
    parser.add_argument("-cashonly", action="store_true", help="Only return cash-rich companies, ignoring companies with other large balance sheet items.")
    parser.add_argument("-dormantonly", action="store_true", help="Only return dormant companies")
    parser.add_argument("-nofilter", action="store_true", help="Don't filter for companies with large balance sheets - include all companies matching the SIC/address search")       

    parser.parse_args(args=None if sys.argv[1:] else ['-h'])

    args = parser.parse_args()
    all_sic_codes = load_json_file(SIC_CODES_FILE, "SIC codes")
    
    if args.cashonly and args.nofilter:
        print("Error: cannot use -cashonly and -nofilter at the same time")
        sys.exit(1)
        
    if args.all and args.nofilter:
        print("Error: cannot return all companies with no filter - that's every UK company")
        sys.exit(1)
        
    # help wth SICs
    if args.sichelp is not None:
        filt = args.sichelp
        if filt == "help":
            show_interesting_sics()
        else:
            f_lower = filt.lower()
            print(f"\nSIC codes matching '{filt}':\n")
            for code, desc in all_sic_codes.items():
                if f_lower == "all" or f_lower in desc.lower():
                    print(f"{code}: {desc}")
        sys.exit(0)
        
    # SIC indexing mode
    if args.index:
        create_index()
        sys.exit(0)

    # Build mode
    if args.build:
        import re
        from collections import defaultdict
        print("Creating accounts lookup table...")
        company_accounts_lookup = {}
        file_types = defaultdict(int)
        pattern = re.compile(r".*_(\d+)_.*\.(\w+)$")
        total_files = 0
        for _,_,files in os.walk(DATA_DIRECTORY):
            total_files += len(files)
        with tqdm(total=total_files, desc="Scanning data directory") as pbar:
            for root,_,files in os.walk(DATA_DIRECTORY):
                for f in files:
                    m = pattern.match(f)
                    if m:
                        cnum = m.group(1)
                        ext = m.group(2)
                        file_types[ext] += 1
                        company_accounts_lookup[cnum] = os.path.join(root,f)
                    pbar.update(1)
        if file_types.get("html",0) == 0:
            print("No .html iXBRL found. Did you unzip them to the right place?")
            sys.exit(1)
        with open(ACCOUNTS_LOOKUP_TABLE_FILE,"w",encoding="utf-8") as j:
            json.dump(company_accounts_lookup, j, indent=4)
        print("Done building. Some files not html:", dict(file_types))
        sys.exit(0)

    # If not building or listing, we do an actual search.
    if not os.path.exists(ACCOUNTS_LOOKUP_TABLE_FILE):
        sys.exit("Error: need to run with -build first, to create accounts lookup.")

    lookup_table = load_json_file(ACCOUNTS_LOOKUP_TABLE_FILE, "Accounts lookup table")

    # Possibly limit the size for debugging
    if DEBUG_LIMIT:
        # If -all, we'll limit the dictionary
        # If -sic/-address, we'll eventually limit the search results anyway
        lookup_table = dict(list(lookup_table.items())[:DEBUG_LIMIT])

    if args.all:

        all_companies = []
        for cnum in lookup_table.keys():
            all_companies.append({
                "CompanyNumber": cnum,
                "CompanyName": "",  # will be overwritten by iXBRL name if found
                "Data": {}  # no CSV data for -all
            })

        results = find_all_cash_rich_companies_fast(lookup_table)
        export_results(results, f"All companies with cash > £{CASH_ALERT_VALUE/1e6:.0f}m", "all")

    elif args.sic is not None:
        sic_help = f"Invalid SIC. Use -sichelp to show some example SICs, or search with (e.g.) -sichelp bank to show SIC codes matching the word 'bank'."
        
        if not args.sic:
            sys.exit(sic_help)
            
        sic_list = args.sic.split("-")
        
        invalid = [s for s in sic_list if s not in all_sic_codes]
        if invalid:
            sys.exit(sic_help)
            
        companies = search_for_sic_code_matches(sic_list, all_sic_codes)
        companies = filter_for_large_companies(companies, lookup_table, cash_only=args.cashonly, nofilter=args.nofilter)
        companies = filter_for_reg_status(companies, args.reg)
        companies = filter_for_dormant(companies, args.dormantonly)

        if len(sic_list) == 1:
            s = sic_list[0]
            html_title = f"Results for {s}: {all_sic_codes[s]}"
        else:
            html_title = "Results for " + ", ".join(sic_list)

        descriptor = "-".join(sic_list)
        export_results(companies, html_title, descriptor)

    elif args.address:
        
        companies = search_for_address_matches(args.address)
        companies = filter_for_large_companies(companies, lookup_table, cash_only=args.cashonly, nofilter=args.nofilter)
        companies = filter_for_reg_status(companies, args.reg)
        companies = filter_for_dormant(companies, args.dormantonly)
        
        descriptor = args.address.lower().replace(" ", "_")
        export_results(companies, f"Large companies at '{args.address}'", descriptor)


if __name__ == "__main__":
    main()
