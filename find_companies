#!/usr/bin/env python3
# -----------------------------------------------------------------------------
# Copyright (c) 2025
# Licensed under the GNU General Public License, version 2
# -----------------------------------------------------------------------------

import csv
import json
import os
import sys
import argparse
import time
import requests
import re
import sqlite3

from tqdm import tqdm
from jinja2 import Environment, FileSystemLoader, select_autoescape
from lxml import etree

# Local settings
from companies_house_settings import (
    scp_destinations,
    fca_user,
    fca_api_key,
    companies_house_snapshot_file,
    CASH_ALERT_VALUE,
    OTHER_ALERT_VALUE
)

DATA_DIRECTORY = "companies_house_data"
COMPANIES_HOUSE_SNAPSHOT = os.path.join(DATA_DIRECTORY, companies_house_snapshot_file)
SIC_CODES_FILE = "sic_codes.json"
ACCOUNTS_LOOKUP_TABLE_FILE = os.path.join(DATA_DIRECTORY, "company_accounts_lookup.json") 
INDEX_DB_FILE = os.path.join(DATA_DIRECTORY, "index_of_company_SICs.db")

COMPANIES_HOUSE_URL = "https://find-and-update.company-information.service.gov.uk/company/{}"
LOGO_URL = "https://taxpolicy.org.uk/wp-content/uploads/elementor/thumbs/logo-in-banner-tight-1-quqdc4qw34zmeg68ggs0psp29qcsftxn2pptrt1h6m.png"
LOGO_LINK = "https://taxpolicy.org.uk"


NUM_WORKER_THREADS = 1000
DEBUG_LIMIT = None  # e.g. set to 1000 for debugging. None for no limit

def load_json_file(filepath, description):
    """Load JSON from a file and exit if not found/invalid."""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        sys.exit(f"Error: Failed to load '{filepath}'. Ensure it exists and is valid JSON.")
        

def extract_ixbrl_data(ixbrl_path):
    """Extract 'financials', 'dormant', and 'name' from an iXBRL file."""
    data = {"financials": {}, "dormant": None, "name": None}
    try:
        tree = etree.parse(ixbrl_path)
    except Exception as e:
        print(f"Error parsing {ixbrl_path}: {e}")
        return data

    try:
        # Extract nonFraction elements
        non_fracs = tree.xpath('//*[local-name()="nonFraction"]')
        data["financials"] = {
            elem.get("name", "").split(":", 1)[-1]: ''.join(elem.itertext()).strip()
            for elem in non_fracs
        }
    except Exception as e:
        print(f"Error extracting nonFraction from {ixbrl_path}: {e}")

    try:
        # Extract nonNumeric elements for 'dormant' and 'name'
        non_num = tree.xpath(
            '//*[local-name()="nonNumeric" and '
            '(substring-after(@name, ":")="EntityDormantTruefalse" or '
            'substring-after(@name, ":")="EntityCurrentLegalOrRegisteredName")]'
        )
        for elem in non_num:
            local_tag = elem.get("name", "").split(":", 1)[-1]
            val = ''.join(elem.itertext()).strip()
            if local_tag == "EntityDormantTruefalse":
                data["dormant"] = "Yes" if val.lower() == "true" else ""
            elif local_tag == "EntityCurrentLegalOrRegisteredName":
                data["name"] = val
    except Exception as e:
        print(f"Error extracting nonNumeric from {ixbrl_path}: {e}")

    return data


def filter_for_large_companies(companies, lookup_table, cash_only=False, nofilter=False):
    if nofilter:
        print("-nofilter - returning all matching companies without filtering.")
        return [
            {
                "CompanyName": co.get("CompanyName", "").strip(),
                "CompanyNumber": co.get("CompanyNumber", "").strip(),
                "SIC1": co["Data"].get("SICCode.SicText_1", ""),
                "SIC2": co["Data"].get("SICCode.SicText_2", ""),
                "SIC3": co["Data"].get("SICCode.SicText_3", ""),
                "SIC4": co["Data"].get("SICCode.SicText_4", ""),
                "Cash": "",
                "SuspectData": [],
                "Dormant": ""
            }
            for co in companies
        ]

    suspect_companies = []
    for company in tqdm(companies, desc="Filtering for large balance sheets"):
        regulatory_status = company.get("regulatory_status", [])
        if regulatory_status:
            continue

        ixbrl_file_path = lookup_table.get(company.get("CompanyNumber", "").strip())
        if not ixbrl_file_path:
            continue

        data = extract_ixbrl_data(ixbrl_file_path)
        financials = data.get("financials", {})
        cash = max(
            (int(v.replace(",", "")) for k, v in financials.items() if "cash" in k.lower() and v.replace(",", "").isdigit()),
            default=0
        )
        suspect_data = [
            (k, float(v.replace(",", "")))
            for k, v in financials.items()
            if "cash" not in k.lower() and v.replace(",", "").isdigit() and float(v.replace(",", "")) > OTHER_ALERT_VALUE
        ] if not cash_only else []

        if cash > CASH_ALERT_VALUE or suspect_data:
            suspect_companies.append({
                "CompanyName": company.get("CompanyName", "").strip(),
                "CompanyNumber": company.get("CompanyNumber", "").strip(),
                "SIC1": company["Data"].get("SICCode.SicText_1", ""),
                "SIC2": company["Data"].get("SICCode.SicText_2", ""),
                "SIC3": company["Data"].get("SICCode.SicText_3", ""),
                "SIC4": company["Data"].get("SICCode.SicText_4", ""),
                "Cash": cash,
                "SuspectData": suspect_data,
                "Dormant": data.get("dormant", "")
            })

    suspect_companies.sort(key=lambda x: x["Cash"], reverse=True)
    filter_msg = "with large cash balances (run with -cashonly)" if cash_only else "with large balance sheets"
    print(f"Found {len(suspect_companies)} companies {filter_msg}")
    return suspect_companies

def filter_for_reg_status(companies, reg):
    if reg:
        companies = add_reg_status(companies)
        companies = remove_regulated_companies(companies)
        
    return companies

def search_fca_register(firm_name, progress_info, max_retries=3, backoff_factor=2):
    """
    Searches the FCA register for `firm_name`, returns a list of matching data dicts.
    """
    url = "https://register.fca.org.uk/services/V0.1/Search"
    headers = {
        "x-auth-email": fca_user,
        "x-auth-key": fca_api_key,
        "Content-Type": "application/json"
    }
    params = {"q": firm_name, "type": "firm"}

    wait_time = 1
    for attempt in range(1, max_retries + 2):
        try:
            print(f"{progress_info} '{firm_name}'" + (f"(attempt {attempt}/{max_retries+1})" if attempt > 1 else ""))
            r = requests.get(url, params=params, headers=headers, timeout=10)
            if r.status_code == 200:
                data = r.json().get("Data")
                if isinstance(data, list):
                    # print(f"Found {len(data)} record(s).")
                    return data
                else:
                    # print(f"No data found.")
                    return []
            elif r.status_code == 429:
                print("Warning: Rate limit. Sleeping 65s...")
                time.sleep(65)
            elif 500 <= r.status_code < 600: 
                print(f"Server error {r.status_code} for '{firm_name}'. Wait {wait_time}s...")
                time.sleep(wait_time)
                wait_time *= backoff_factor
            else:
                print(f"Error: HTTP {r.status_code} for '{firm_name}'. Skipping.")
                return []
        except requests.exceptions.Timeout:
            print(f"Timeout for '{firm_name}'. Wait {wait_time}s...")
            time.sleep(wait_time)
            wait_time *= backoff_factor
        except requests.exceptions.RequestException as e:
            print(f"Request error for '{firm_name}': {e}")
            time.sleep(wait_time)
            wait_time *= backoff_factor
    print(f"Failed to retrieve data for '{firm_name}' after {max_retries} attempts.")
    return []

def add_reg_status(companies):
    """
    Adds regulatory status to each company and prunes the regulatory information
    to include only entries that match the company's normalized name.
    """
    for i, co in enumerate(companies, 1):
        cname = co.get("CompanyName", "").strip()
        cnum = co.get("CompanyNumber", "").strip()
        if not cname or not cnum:
            print(f"[{i}/{len(companies)}] Missing name/number - skipping.")
            co["regulatory_status"] = []
            continue

        # Fetch regulatory status
        reg_status = search_fca_register(cname, f"[{i}/{len(companies)}]")
        
        # Prune regulatory status
        norm_cname = normalize_name(cname)
        pruned_status = []
        for entry in reg_status:
            entry_name = entry.get("Name", "")
            norm_entry_name = normalize_name(entry_name)
            if norm_entry_name == norm_cname:
                pruned_status.append(entry)
                print(f"{cname} is a regulated firm")
            else:
                pass
                # print(f"{cname}: Deleting regulatory entry '{entry_name}' as it does not match normalized name.")
        
        co["regulatory_status"] = pruned_status
    return companies


def normalize_name(name):
    name = name.lower().strip()
    name = re.sub(r"\s*\([^)]*\)\s*", " ", name).strip()  # Removes parentheses and contents
    name = name.replace("limited", "ltd").replace(".", "").replace(",", "")
    return name

def remove_regulated_companies(companies):
    init_count = len(companies)
    unreg = [c for c in companies if not c.get("regulatory_status")]
    print(f"Filtered out {init_count - len(unreg)} regulated companies. {len(unreg)} remain.")
    return unreg

def create_html(suspect_companies, table_title, descriptor):
    filename = f"output/results-{descriptor}.html"
    os.makedirs("output", exist_ok=True)

    env = Environment(
        loader=FileSystemLoader("."),
        autoescape=select_autoescape(["html", "xml"])
    )

    def format_number(val):
        try:
            return "{:,}".format(float(val))
        except:
            return val
    env.filters["format_number"] = format_number

    template_str = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>TABLE_TITLE</title>
    <!-- Google Fonts: Poppins -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet">
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- DataTables CSS -->
    <link rel="stylesheet" href="https://cdn.datatables.net/1.13.4/css/dataTables.bootstrap5.min.css">
    <!-- Favicon -->
    <link rel="icon" href="{{ logo_url }}" sizes="32x32" type="image/jpeg">

    <style>
        body {
            font-family: 'Poppins', sans-serif;
            color: #000000;
        }
        .container {
            margin-top: 50px;
        }
        /* Header Styling */
        .header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            background-color: #1133AF; 
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #FFFFFF;
            margin: 0;
            font-weight: 600;
        }
        /* Logo Styling */
        .logo {
            height: 60px;
        }
        /* Table Header Styling */
        table.dataTable thead th {
            background-color: #1133AF;
            color: white;
        }
        /* Table Row Hover and Zebra Striping */
        table.dataTable tbody tr:nth-child(odd) {
            background-color: #f9f9f9;
        }
        table.dataTable tbody tr:hover {
            background-color: #D3D3D3 !important;
        }
        /* Links Styling */
        a {
            color: #1133AF;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        /* DataTables Length and Filter Styling */
        .dataTables_wrapper .dataTables_length,
        .dataTables_wrapper .dataTables_filter {
            color: #000000;
        }
        /* Ensure the Cash column is right-aligned for better readability */
        td:nth-child(4), th:nth-child(4) {
            text-align: left;
        }
        /* SIC Cell Styling */
        .sic-cell span {
            display: block;
            max-width: 300px; /* Adjust as needed */
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
            cursor: pointer;
        }
        /* Added CSS for wrapping long suspect items */
        .item-cell {
            max-width: 200px; /* Adjust the width as needed */
            word-wrap: break-word;
            white-space: normal;
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="header" id="main-header">
            <h1>TABLE_TITLE</h1>
            <a href="{{ logo_link }}" target="_blank">
                <img src="{{ logo_url }}" alt="Company Logo" class="logo">
            </a>
        </div>

        <table id="suspectCompaniesTable" class="table table-striped table-bordered" style="width:100%">
            <thead>
                <tr>
                    <th>Company Name</th>
                    <th>Company Number</th>
                    <th>Dormant</th>
                    {% if descriptor != "all" %}
                        <th>SICs</th>
                    {% endif %}
                    <th>Cash</th>
                    {% for i in range(1, max_suspects + 1) %}
                        <th>Item {{ i }}</th>
                        <th>Value {{ i }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody>
                {% for co in companies %}
                <tr>
                    <td>{{ co.CompanyName }}</td>
                    <td>
                        <a href="{{ ch_url.format(co.CompanyNumber) }}" target="_blank">
                            {{ co.CompanyNumber }}
                        </a>
                    </td>
                    <td>{{ co.Dormant }}</td>
                    
                    {% if descriptor != "all" %}
                        <td class="sic-cell">
                            {% for sic in [co.SIC1, co.SIC2, co.SIC3, co.SIC4] %}
                                {% if sic %}
                                    <span data-bs-toggle="tooltip" data-bs-placement="top" title="{{ sic }}">
                                        {{ sic[:25] }}{% if sic|length > 25 %}...{% endif %}
                                    </span>
                                {% endif %}
                            {% endfor %}
                        </td>
                    {% endif %}

                    <td data-sort="{{ co.Cash }}">{{  "{:,.0f}".format(co.Cash|int)  }}</td>

                    {% for item, val in co.SuspectData %}
                        <td class="item-cell">{{ item }}</td>
                        <td>{{ "{:,.0f}".format(val|int) }}</td>
                    {% endfor %}

                    {% for _ in range(max_suspects - co.SuspectData|length) %}
                        <td></td>
                        <td></td>
                    {% endfor %}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>

    <!-- jQuery -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <!-- Bootstrap JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <!-- DataTables JS -->
    <script src="https://cdn.datatables.net/1.13.4/js/jquery.dataTables.min.js"></script>
    <script src="https://cdn.datatables.net/1.13.4/js/dataTables.bootstrap5.min.js"></script>
    <!-- Scroller Extension JS -->
    <script src="https://cdn.datatables.net/scroller/2.1.2/js/dataTables.scroller.min.js"></script>

    <script>
        $(document).ready(function() {
            $('#suspectCompaniesTable').DataTable({
                scrollY: "800px",
                scrollX: true,
                scroller: true,
                paging: false,
                searching: false,
                order: [[ 3, "desc" ]],
                lengthMenu: [10, 25, 50, 100, 4021],
                columnDefs: [
                    {
                        targets: 3,
                        type: "num",
                        render: function (data, type, row) {
                            // So the table sorts numerically by the raw value in data-sort
                            if (type === 'sort') {
                                return data;
                            }
                            return data;
                        }
                    }
                ],
                language: {
                    search: "Filter records:",
                    lengthMenu: "Show _MENU_ entries",
                    info: "Showing _START_ to _END_ of _TOTAL_ companies",
                    infoEmpty: "No companies available",
                    infoFiltered: "(filtered from _MAX_ total companies)"
                }
            });

            // Initialize Bootstrap tooltips
            var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'))
            tooltipTriggerList.map(function (tooltipTriggerEl) {
                return new bootstrap.Tooltip(tooltipTriggerEl)
            });
        });

        // Hide the header if embedded in an iframe
        function checkIfEmbedded() {
            if (window.self !== window.top) {
                var header = document.getElementById('main-header');
                if (header) {
                    header.style.display = 'none';
                }
            }
        }
        document.addEventListener("DOMContentLoaded", checkIfEmbedded);
    </script>
</body>
</html>
"""
    template_str = template_str.replace("TABLE_TITLE", table_title)
    template = env.from_string(template_str)

    if "SuspectData" in suspect_companies[0]:
        max_suspects = max((len(c["SuspectData"]) for c in suspect_companies), default=0)
    else:
        max_suspects = 0
        
    html_out = template.render(
        companies=suspect_companies,
        max_suspects=max_suspects,
        ch_url=COMPANIES_HOUSE_URL,
        logo_url=LOGO_URL,
        logo_link=LOGO_LINK,
        descriptor=descriptor
    )
    try:
        with open(filename, "w", encoding="utf-8") as f:
            f.write(html_out)
    except IOError as e:
        print(f"Error writing {filename}: {e}")
    return filename

def create_csv(suspect_companies, descriptor):
    filename = f"output/results-{descriptor}.csv"
    os.makedirs("output", exist_ok=True)
    max_items = max((len(c.get("SuspectData", [])) for c in suspect_companies), default=0)
    headers = ["Company Name", "Company Number", "Cash"]
    if descriptor != "all":
        headers += ["SIC1", "SIC2", "SIC3", "SIC4"]
    headers += [f"Item {i+1}" for i in range(max_items)] + [f"Value {i+1}" for i in range(max_items)]

    with open(filename, "w", encoding="utf-8", newline="") as out:
        writer = csv.writer(out)
        writer.writerow(headers)
        for co in suspect_companies:
            row = [
                co["CompanyName"], co["CompanyNumber"], co["Cash"]
            ]
            if descriptor != "all":
                row += [co.get(f"SIC{i}", "") for i in range(1, 5)]
            row += [item for pair in co.get("SuspectData", []) for item in pair]
            # Pad with empty strings if necessary
            row += [""] * (2 * (max_items - len(co.get("SuspectData", []))))
            writer.writerow(row)
    return filename


def export_results(companies, html_title, descriptor, run_scp = True):
    if not companies:
        print("\nNo results - nothing to export.")
        return
    
    # sanitise filename
    descriptor = re.sub(r'[\\\/:*?"<>|]', '_', descriptor)
    
    html_file = create_html(companies, html_title, descriptor)
    csv_file = create_csv(companies, descriptor)
    
    if scp_destinations and run_scp:
        print("\nUploading:")
        for dest in scp_destinations:
            os.system(f"scp {html_file} {dest}")
            os.system(f"scp {csv_file} {dest}")
    else:
        print(f"Exported to:\n  {html_file}\n  {csv_file}\n")



def indexed_search(search_type, search_terms, search_description):

    conn = sqlite3.connect(INDEX_DB_FILE)
    cursor = conn.cursor()
    
    cursor.execute("SELECT COUNT(*) FROM Companies")
    total_companies = cursor.fetchone()[0]
    print(f"Searching {total_companies:,} companies for {search_description}")

    if search_type == "address":
        # Use LIKE operator with wildcards for partial matching
        query = '''
            SELECT CompanyNumber, CompanyName, Data
            FROM Companies
            WHERE FullAddress LIKE ?
        '''
        search_pattern = f"%{search_terms.strip().lower()}%"
        cursor.execute(query, (search_pattern,))
        
    elif search_type == "SIC":
        placeholders = ','.join(['?'] * len(search_terms))
        query = f'''
            SELECT DISTINCT c.CompanyNumber, c.CompanyName, c.Data
            FROM SICCodes s
            JOIN Companies c ON s.CompanyNumber = c.CompanyNumber
            WHERE s.SICCode IN ({placeholders})
        '''
        cursor.execute(query, tuple(search_terms))
    else:
        print("Error - unknown indexed search type")
        sys.exit(1)
        
    rows = cursor.fetchall()

    matched_companies = []
    for row in rows:
        cnum, cname, data_json = row
        data = json.loads(data_json)
                    
        matched_companies.append({
            "CompanyName": cname,
            "CompanyNumber": cnum,
            "Data": data,
        })

    conn.close()

    print(f"Found {len(matched_companies):,} companies with matching {search_type}.\n")
    return matched_companies




######################################
# multithreaded function for finding ALL companies
# different approach from standard search because it simply goes through all accounts in lookup table

def search_all_companies_for_large_cash_holdings(lookup_table, args):
    
    # lazy loading of large imports we only use for -all searches
    import threading
    import queue
    import concurrent.futures
    
    if args.nofilter:
        print("Error: cannot return all companies with no filter - that's every UK company")
        sys.exit(1)
    
    def worker(company_number, ixbrl_file_path, result_queue, total_companies):
        if not ixbrl_file_path:
            return

        all_accounts_data = extract_ixbrl_data(ixbrl_file_path)
        
        cash = 0
        for key, value in all_accounts_data["financials"].items():
            if "cash" in key.lower():
                clean_str_value = value.replace(",", "")
                try:
                    value_int = int(clean_str_value)
                except ValueError:
                    return
                cash = max(cash, value_int)
        
        if cash == 0:
            return
        
        company_name = all_accounts_data["name"]
        if cash > CASH_ALERT_VALUE:
            result_queue.put({
                "CompanyNumber": company_number,
                "CompanyName": company_name,
                "Dormant": all_accounts_data["dormant"],
                "Cash": value_int
            })


    
    def collector(result_queue, cash_rich_companies, done_event):
        while not done_event.is_set() or not result_queue.empty():
            try:
                item = result_queue.get(timeout=0.1)
                if item is None:
                    # Sentinel received
                    break
                cash_rich_companies.append(item)
            except queue.Empty:
                continue
    
    total_companies = len(lookup_table)
    print(f"Searching all {total_companies:,} company accounts for cash-rich companies ...")
    
    cash_rich_companies = []
    result_queue = queue.Queue()
    done_event = threading.Event()

    # Start the collector thread
    collector_thread = threading.Thread(target=collector, args=(result_queue, cash_rich_companies, done_event))
    collector_thread.start()

    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKER_THREADS) as executor:
        futures = []
        for company_number, ixbrl_file_path in tqdm(lookup_table.items(), total=total_companies, desc="Searching all companies for high cash balances"):
            futures.append(executor.submit(worker, company_number, ixbrl_file_path, result_queue, total_companies))
        
        # Wait for all futures to complete
        concurrent.futures.wait(futures)

    # All workers are done, signal the collector to finish
    done_event.set()
    collector_thread.join()

    print(f"Found {len(cash_rich_companies)} with large balance sheets")
    return cash_rich_companies

######################


def build_accounts_lookup_table():
    
    from collections import defaultdict  # lazy loading
    
    # Remove existing lookup table if exists
    if os.path.exists(ACCOUNTS_LOOKUP_TABLE_FILE):
        print(f"Building lookup table; replacing existing lookup table {ACCOUNTS_LOOKUP_TABLE_FILE}...")
        os.remove(ACCOUNTS_LOOKUP_TABLE_FILE)
    else:
        print(f"Building new accounts lookup table at {ACCOUNTS_LOOKUP_TABLE_FILE}...")
    
    company_accounts_lookup = {}
    file_types = defaultdict(int)
    pattern = re.compile(r".*_(\d+)_.*\.(\w+)$")
    total_files = 0
    for _,_,files in os.walk(DATA_DIRECTORY):
        total_files += len(files)
    with tqdm(total=total_files, desc="Scanning data directory") as pbar:
        for root,_,files in os.walk(DATA_DIRECTORY):
            for f in files:
                m = pattern.match(f)
                if m:
                    cnum = m.group(1)
                    ext = m.group(2)
                    file_types[ext] += 1
                    company_accounts_lookup[cnum] = os.path.join(root,f)
                pbar.update(1)
    if file_types.get("html",0) == 0:
        print("No .html iXBRL found. Did you unzip them to the right place?")
        sys.exit(1)
    with open(ACCOUNTS_LOOKUP_TABLE_FILE,"w",encoding="utf-8") as j:
        json.dump(company_accounts_lookup, j, indent=4)
    print("Done building. Some files not html:", dict(file_types))
    sys.exit(0)

def create_index(csv_path=COMPANIES_HOUSE_SNAPSHOT, db_path=INDEX_DB_FILE):
    
    if not os.path.exists(csv_path):
        sys.exit(f"Error: CSV file '{csv_path}' not found.")

    # Remove existing DB if exists
    if os.path.exists(db_path):
        print(f"Creating index; replacing existing database {db_path}")
        os.remove(db_path)
    else:
        print(f"Creating index. Database Path: {db_path}")
        
    # Define address fields
    address_fields = [
        "RegAddress.CareOf",
        "RegAddress.POBox",
        "RegAddress.AddressLine1",
        "RegAddress.AddressLine2",
        "RegAddress.PostTown",
        "RegAddress.County",
        "RegAddress.Country",
        "RegAddress.PostCode"
    ]

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Optimize SQLite for faster insertion
    cursor.execute('PRAGMA synchronous = OFF')
    cursor.execute('PRAGMA journal_mode = MEMORY')
    cursor.execute('PRAGMA temp_store = MEMORY')
    cursor.execute('PRAGMA cache_size = 100000')
    cursor.execute('PRAGMA foreign_keys = ON')

    # Create tables
    cursor.execute('''
        CREATE TABLE Companies (
            CompanyNumber TEXT PRIMARY KEY,
            CompanyName TEXT,
            FullAddress TEXT,
            Data TEXT
        )
    ''')

    cursor.execute('''
        CREATE TABLE SICCodes (
            SICCode TEXT,
            CompanyNumber TEXT,
            FOREIGN KEY (CompanyNumber) REFERENCES Companies (CompanyNumber)
        )
    ''')

    # Create index for fast lookup
    cursor.execute('CREATE INDEX idx_sic_code ON SICCodes (SICCode)')
    cursor.execute('CREATE INDEX idx_company_number ON SICCodes (CompanyNumber)')
    cursor.execute('CREATE INDEX idx_full_address ON Companies (FullAddress)')

    conn.commit()

    # Read CSV and populate the database
    with open(csv_path, 'r', newline='', encoding='utf-8') as csvfile:
        # Read the first line to get headers
        first_line = csvfile.readline()
        headers = first_line.strip().split(',')
        # Strip whitespace from each header
        stripped_headers = [header.strip() for header in headers]

        # Initialize DictReader with stripped headers
        reader = csv.DictReader(csvfile, fieldnames=stripped_headers)
        
        # Skip the first line since we've already read the headers
        next(reader, None)

        # Verify required fields
        required_fields = ["CompanyNumber", "CompanyName", "SICCode.SicText_1", 
                           "SICCode.SicText_2", "SICCode.SicText_3", "SICCode.SicText_4"]
        missing_fields = [field for field in required_fields if field not in stripped_headers]
        if missing_fields:
            print(f"Error: Missing required fields in CSV: {missing_fields}")
            sys.exit(1)

        # Count total rows for progress bar
        
        with open(csv_path, 'r', encoding='utf-8') as f:
            total_rows = sum(1 for _ in f) - 1  # Exclude header
        
        # Reset file pointer to after the header
        csvfile.seek(0)
        next(csvfile)  # Skip the header line
        reader = csv.DictReader(csvfile, fieldnames=stripped_headers)

        batch_size = 10000
        companies_batch = []
        sics_batch = []
        inserted_companies = 0
        inserted_sics = 0

        with tqdm(total=total_rows, desc="Indexing CSV") as pbar:
            for idx, row in enumerate(reader, start=1):
                company_number = row.get("CompanyNumber", "").strip()
                company_name = row.get("CompanyName", "").strip()                
                full_address = ' '.join([row.get(field, "").strip().lower() for field in address_fields])

                if not company_number:
                    pbar.update(1)
                    continue  # Skip entries without CompanyNumber

                # Serialize row to JSON, handle serialization errors
                try:
                    data_json = json.dumps(row)
                except (TypeError, OverflowError) as e:
                    print(f"JSON Serialization Error for CompanyNumber {company_number}: {e}")
                    data_json = "{}"  # Fallback to empty JSON

                # Store company details
                companies_batch.append((
                    company_number,
                    company_name,
                    full_address,
                    data_json
                ))

                # Extract and store SIC codes
                sic_fields = ["SICCode.SicText_1", "SICCode.SicText_2", 
                              "SICCode.SicText_3", "SICCode.SicText_4"]
                for field in sic_fields:
                    sic_val = row.get(field, "").strip()
                    if sic_val:
                        code = sic_val[:5]
                        if code.isdigit():
                            sics_batch.append((code, company_number))

                # Insert in batches
                if idx % batch_size == 0:
                    try:
                        cursor.executemany('INSERT OR IGNORE INTO Companies VALUES (?, ?, ?, ?)', companies_batch)
                        cursor.executemany('INSERT INTO SICCodes VALUES (?, ?)', sics_batch)
                        conn.commit()
                        inserted_companies += len(companies_batch)
                        inserted_sics += len(sics_batch)
                    except sqlite3.Error as e:
                        print(f"SQLite Error at batch {idx // batch_size}: {e}")
                        conn.rollback()
                    finally:
                        companies_batch.clear()
                        sics_batch.clear()

                pbar.update(1)

        # Insert any remaining records
        if companies_batch or sics_batch:
            try:
                cursor.executemany('INSERT OR IGNORE INTO Companies VALUES (?, ?, ?, ?)', companies_batch)
                cursor.executemany('INSERT INTO SICCodes VALUES (?, ?)', sics_batch)
                conn.commit()
                inserted_companies += len(companies_batch)
                inserted_sics += len(sics_batch)
                
            except sqlite3.Error as e:
                print(f"SQLite Error during final batch insertion: {e}")
                conn.rollback()

    conn.close()
    print(f"Indexing complete. Database saved to '{db_path}'.")

def filter_for_dormant(companies, dormant_arg):
    if dormant_arg:
        companies = [company for company in companies if company.get("Dormant") == "Yes"]
        print(f"Of which {len(companies)} are dormant")
        return companies
    else:
        return companies
    

def return_interesting_sics():
    interesting_sics = """
Some interesting SIC codes:

    64191: Banks
    64110: Central banking
    64192: Building societies
    64301: Activities of investment trusts
    64302: Activities of unit trusts
    64303: Activities of venture and development capital companies
    64304: Activities of open-ended investment companies
    64305: Activities of property unit trusts
    64306: Activities of real estate investment trusts
    64910: Financial leasing
    64921: Credit granting by non-deposit taking finance houses and other specialist consumer credit grantors
    64922: Activities of mortgage finance companies
    64929: Other credit granting n.e.c.
    64991: Security dealing on own account
    64992: Factoring
    64999: Financial intermediation not elsewhere classified
    65110: Life insurance
    65120: Non-life insurance
    65201: Life reinsurance
    65202: Non-life reinsurance
    65300: Pension funding
    66110: Administration of financial markets
    66120: Security and commodity contracts dealing activities
    66220: Activities of insurance agents and brokers
    66300: Fund management activities

    Run with -sichelp TEXT to see all the SIC codes matching TEXT
    or run with -sichelp ALL to see every SIC code.
"""
    return interesting_sics


# runs a batch of all SICs for website
# not very efficient as doesn't cache FCA search results, when one company could appear four times    
def runbatch(target, lookup_table, all_sic_codes):
    print(f"Running batch for {target}")
    iter = 0
    start = time.time()
    
    for SIC in target:
        search_terms = [SIC]
        for cashonly in [True, False]:
            for dormant in [True, False]:
                
                iter += 1
                descriptor = f"{SIC}_{all_sic_codes[SIC].replace(' ', '_')}{'-dormants' if dormant else ''}{'-cashonly' if cashonly else ''}"
                html_title = f"{SIC}: {all_sic_codes[SIC]}{', dormants' if dormant else ''}{', cash-rich' if cashonly else ''}"
                search_description = f"{iter}/{len(target) * 4}- {html_title}"
                print(descriptor)
                print(html_title)
                print(search_description)
                time.sleep(1)
                
                companies = indexed_search("SIC", search_terms, search_description)    
                companies = filter_for_large_companies(companies, lookup_table, cash_only=cashonly, nofilter=False)
                companies = filter_for_dormant(companies, dormant)
                companies = filter_for_reg_status(companies, True)
                
                export_results(companies, html_title, descriptor, run_scp=False)
                
    print(f"\nCompleted batch run - took {int(start - time.time())}s")

def setup_argument_parser():
    class CustomFormatter(argparse.RawTextHelpFormatter, argparse.ArgumentDefaultsHelpFormatter):
        pass
    
    parser = argparse.ArgumentParser(
        description=(
            "taxpolicy.org.uk - Companies House fraudulent company accounts finder, v0.3\n\n"
            f"Find companies matching certain criteria with either "
            f"cash > £{CASH_ALERT_VALUE/1e6:.0f}m, or "
            f"other balances > £{OTHER_ALERT_VALUE/1e6:.0f}m.\n"
            f"(c) Dan Neidle, Tax Policy Associates 2025\n"
            f"Licensed under the GNU General Public License, version 2\n"
            
            
        ), formatter_class=CustomFormatter
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("-sichelp", nargs="?", const="help", help="Show available SIC codes (optionally filter).", default=argparse.SUPPRESS)
    group.add_argument("-sic", help="Search by SIC code(s).", default=argparse.SUPPRESS)
    group.add_argument("-address", help="Search by partial address.", default=argparse.SUPPRESS)
    group.add_argument("-all", action="store_true", help="Search accounts of *all* companies for high cash holdings (will take 40 mins+).", default=argparse.SUPPRESS)
    group.add_argument("-buildlookup", action="store_true", help="Build the accounts lookup table.", default=argparse.SUPPRESS)
    group.add_argument("-buildindex", action="store_true", help="Build an index for SIC/address searches. Will take a few minutes.", default=argparse.SUPPRESS)
    group.add_argument("-runbatchinteresting", action="store_true", help="Run on all 'interesting' SICs", default=argparse.SUPPRESS)
    group.add_argument("-runbatchall", action="store_true", help="Run on all SICs", default=argparse.SUPPRESS)

    parser.add_argument("-reg", action="store_true", help="Filter out regulated companies.")
    parser.add_argument("-cashonly", action="store_true", help="Only return cash-rich companies, ignoring companies with other large balance sheet items.")
    parser.add_argument("-dormantonly", action="store_true", help="Only return dormant companies")
    parser.add_argument("-nofilter", action="store_true", help="Don't filter for companies with large balance sheets - include all companies matching the SIC/address search")       

    parser.parse_args(args=None if sys.argv[1:] else ['-h'])
    
    return parser

def validate_dependencies():

    if not os.path.exists(ACCOUNTS_LOOKUP_TABLE_FILE):
        sys.exit("Error: need to run with -buildlookup first, to create accounts lookup.")
    
    if not os.path.exists(INDEX_DB_FILE):
        sys.exit("Error: need to run with -buildindex first, to index SIC codes and addresses.")


def handle_sic_help(filt, all_sic_codes):
    if filt == "help":
        print(return_interesting_sics())
    else:
        f_lower = filt.lower()
        print(f"\nSIC codes matching '{filt}':\n")
        for code, desc in all_sic_codes.items():
            if f_lower == "all" or f_lower in desc.lower():
                print(f"{code}: {desc}")
                
def validate_sic_search(args, all_sic_codes):
    search_terms = args.sic.split("-")
    
    invalid = [s for s in search_terms if s not in all_sic_codes]
    if invalid:
        sys.exit(f"Invalid SIC. Use -sichelp to show some example SICs, or search with (e.g.) -sichelp bank to show SIC codes matching the word 'bank'.")
            
    if len(search_terms) == 1:
        s = search_terms[0]
        html_title = f"Results for {s}: {all_sic_codes[s]}"
    else:
        html_title = "Results for " + ", ".join(search_terms)
        
    return search_terms, html_title


def main():
    
    parser = setup_argument_parser()

    args = parser.parse_args()
    all_sic_codes = load_json_file(SIC_CODES_FILE, "SIC codes")
    
    if hasattr(args, "sichelp"):
        handle_sic_help(args.sichelp, all_sic_codes)
        sys.exit(0)

    elif args.cashonly and args.nofilter:
        print("Error: cannot use -cashonly and -nofilter at the same time")
        sys.exit(1)
        
    # SIC indexing mode
    elif hasattr(args, "buildindex"):
        create_index()
        sys.exit(0)

    # Build mode
    elif hasattr(args, "buildlookup"):
        build_accounts_lookup_table()
        sys.exit(0)
        
    validate_dependencies()

    lookup_table = load_json_file(ACCOUNTS_LOOKUP_TABLE_FILE, "Accounts lookup table")

    # Possibly limit the size for debugging
    if DEBUG_LIMIT:
        print(f"\n***\nWARNING: debugging, only searching first {DEBUG_LIMIT} companies!\n***\n")
        lookup_table = dict(list(lookup_table.items())[:DEBUG_LIMIT])

    if hasattr(args, "all"):
        companies = search_all_companies_for_large_cash_holdings(lookup_table, args)
        companies = filter_for_dormant(companies, args.dormantonly)
        export_results(companies, f"All companies with cash > £{CASH_ALERT_VALUE/1e6:.0f}m", f"all{'-dormant' if args.dormantonly else ''}")
        
    elif hasattr(args, "runbatchall"):
        all_sics = list(all_sic_codes.keys())
        runbatch(all_sics, lookup_table, all_sic_codes)
        sys.exit(0)
        
    elif hasattr(args, "runbatchinteresting"):
        interesting_sics = re.findall(r'\b\d{5}\b', return_interesting_sics())
        runbatch(interesting_sics, lookup_table, all_sic_codes)
        sys.exit(0)

    # prepare SIC search
    elif hasattr(args, "sic"):
        
        search_terms, html_title = validate_sic_search(args, all_sic_codes)
        descriptor = "-".join(search_terms)
        search_type = "SIC"
        search_description = "SIC codes " + ", ".join(f"{code}: {all_sic_codes[code]}" for code in search_terms)

    # prepare address search
    elif hasattr(args, "address"):
        
        descriptor = args.address.lower().replace(" ", "_")
        html_title = f"Large companies at '{args.address}'"
        search_terms, search_type, search_description = args.address, "address", f"addresses containing '{args.address}' (case insensitive)"
        
    # do searching and filtering for address/SIC search
    if hasattr(args, "address") or hasattr(args, "sic"):
        
        companies = indexed_search(search_type, search_terms, search_description)    
        companies = filter_for_large_companies(companies, lookup_table, cash_only=args.cashonly, nofilter=args.nofilter)
        companies = filter_for_dormant(companies, args.dormantonly)
        companies = filter_for_reg_status(companies, args.reg)
        descriptor = f"{descriptor}{'-dormants' if args.dormantonly else ''}{'-cashonly' if args.cashonly else ''}{'-nofilter' if args.nofilter else ''}{'-unregs' if args.reg else ''}"
        export_results(companies, html_title, descriptor)

if __name__ == "__main__":
    main()


